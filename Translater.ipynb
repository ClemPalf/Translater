{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translater\n",
    "\n",
    "Hello everyone, \n",
    "\n",
    "Welcome to this little notebook which will help you build your own translator!\n",
    "Follow the instructions provided in the README as we go!\n",
    "\n",
    "\n",
    "In our example, we will translate from English to German,but feel free to select the language you want.\n",
    "![Flags overview image](Images/germany_uk_flags.png)\n",
    "\n",
    "To this regard, I will refer as the 'English sentences' the sentences of the language we want to translate, and the 'German sentences' the targeted language sentences. \n",
    "\n",
    "\n",
    "Let's first import all the libraries we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Layer, Softmax\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential, load_model \n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Input, Embedding, LSTM\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "NUM_EXAMPLES = 1000     #200000                # HERE, YOU MIGHT WANT TO REDUCE THE NB OF EXAMPLES IF TRAINING THE MODEL TAKE TOO LONG.\n",
    "data_examples = []\n",
    "with open('deu.txt', 'r', encoding='utf8') as f:       # HERE, ENTER THE NAME OF YOUR TEXT FILE.\n",
    "    for line in f.readlines():\n",
    "        if len(data_examples) < NUM_EXAMPLES:\n",
    "            data_examples.append(line)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions will help simplify special characters, feel free to add more.\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"ü\", 'ue', sentence)\n",
    "    sentence = re.sub(r\"ä\", 'ae', sentence)\n",
    "    sentence = re.sub(r\"ö\", 'oe', sentence)\n",
    "    sentence = re.sub(r'ß', 'ss', sentence)\n",
    "    \n",
    "    sentence = unicode_to_ascii(sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r\"[^a-z?.!,']+\", \" \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Preprocess the data\n",
    "\n",
    "Here we will:\n",
    "* Create separate lists of English and German sentences, and preprocess them using the `preprocess_sentence` function created above.\n",
    "* Add a special `\"<start>\"` and `\"<end>\"` token to the beginning and end of every German sentence.\n",
    "* Tokenize the German sentences, ensuring that no character filters are applied.\n",
    "* Pad the end of the tokenized German sequences with zeros, and batch the complete set of sequences into one numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each sentences \n",
    "\n",
    "English_sentences = []\n",
    "German_sentences = []\n",
    "\n",
    "for i in data_examples:\n",
    "    \n",
    "    English = re.search(r\"^[^\\t]*[\\.|\\!|\\?]\", i)\n",
    "    German = re.search(r\"\\t[^\\t]*[\\.|\\!|\\?]\", i)\n",
    "    \n",
    "    if English == None or German == None:\n",
    "        continue\n",
    "    \n",
    "    English_sentences.append(English[0])\n",
    "    German_sentences.append(German[0][1:])\n",
    "\n",
    "# Preprocess the data with the above functions \n",
    "\n",
    "English_preprocessed = [preprocess_sentence(i) for i in English_sentences]\n",
    "German_preprocessed = [preprocess_sentence(i) for i in German_sentences]\n",
    "\n",
    "# Add <start> and <end> in each sentences of the targeted language\n",
    "\n",
    "German_preprocessed_2 = [\"<start> \" + i + \" <end>\" for i in German_preprocessed]\n",
    "\n",
    "# Tokenize the targeted language sentences\n",
    "\n",
    "Tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None, filters='', split=' ', \n",
    "                                                         char_level=False, oov_token= None)\n",
    "\n",
    "Tokenizer.fit_on_texts(German_preprocessed_2)\n",
    "\n",
    "German_tokenized = Tokenizer.texts_to_sequences(German_preprocessed_2)\n",
    "\n",
    "\n",
    "# Get the number of unique word (useful later)\n",
    "\n",
    "word_index = Tokenizer.word_index\n",
    "num_german_tokens = len(word_index)\n",
    "tokenizer_config = Tokenizer.get_config()\n",
    "\n",
    "# Pad the targeted language sequences\n",
    "\n",
    "German_padded = tf.keras.preprocessing.sequence.pad_sequences(German_tokenized, \n",
    "                                                              maxlen=len(max(German_tokenized, key=len)),\n",
    "                                                              padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Load the embedding layer\n",
    "\n",
    "As for many NLP applications, we will need to embed our inputs. In this project, let's use a pre-trained module from TensorFlow Hub. The URL for the module is https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1. \n",
    "This embedding takes a batch of text tokens in a 1-D tensor of strings as input. It then embeds the separate tokens into a 128-dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\", output_shape=[128], input_shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Prepare the training and validation Datasets.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "* Create a random training and validation set split of the data, reserving 20% of the data for validation.\n",
    "* Load the training and validation sets into a tf.data.Dataset object, passing in a tuple of English and German data for both training and validation sets.\n",
    "* Create a function to map over the datasets that splits each English sentence at spaces, and apply it to both Dataset objects using the map method. \n",
    "* Create and apply a function to map over the datasets that embeds each sequence of English words using the loaded embedding layer/model. \n",
    "* Create and apply a function to filter out dataset examples where the English sentence is more than 13 (embedded) tokens in length. \n",
    "* Create and apply a function to map over the datasets that pads each English sequence of embeddings with some distinct padding value before the sequence, so that each sequence is length 13. \n",
    "* Batch both training and validation Datasets with a batch size of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split randomly training and validation examples, with 20 percent for validation.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(English_preprocessed, German_padded, test_size=0.2, \n",
    "                                         random_state=None, shuffle=True, stratify=None)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Pass training and test sets into dataset\n",
    "\n",
    "Training_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "Test_dataset = tf.data.Dataset.from_tensor_slices((X_test,  Y_test))\n",
    "\n",
    "# Split function\n",
    "\n",
    "def split_English_sentences(English, German):\n",
    "    \n",
    "    English = tf.strings.split(English)\n",
    "    \n",
    "    return English, German \n",
    "\n",
    "# Map this function over the datasets\n",
    "Training_dataset = Training_dataset.map(split_English_sentences)\n",
    "Test_dataset = Test_dataset.map(split_English_sentences)\n",
    "\n",
    "# Embedding function\n",
    "\n",
    "def Embedd_English_sentences(English, German):\n",
    "    \n",
    "    English = embedding_layer(English)\n",
    "    \n",
    "    return English, German \n",
    "\n",
    "# Map this function over the datasets\n",
    "Training_dataset = Training_dataset.map(Embedd_English_sentences)\n",
    "Test_dataset = Test_dataset.map(Embedd_English_sentences)\n",
    "\n",
    "# Filtering function\n",
    "\n",
    "def Filter_English_sentences(dataset):\n",
    "    \n",
    "    def filter_func(English, German):\n",
    "\n",
    "        if len(English) <= 13:\n",
    "            res = True\n",
    "        else:\n",
    "            res = False\n",
    "\n",
    "        return res\n",
    "\n",
    "    filtered_dataset = dataset.filter(filter_func)\n",
    "\n",
    "    return filtered_dataset\n",
    "\n",
    "# Apply the function to the datasets\n",
    "Training_dataset = Filter_English_sentences(Training_dataset)\n",
    "Test_dataset = Filter_English_sentences(Test_dataset)\n",
    "\n",
    "def Pad_English_sentences(English, German):\n",
    "    \n",
    "    \n",
    "    paddings = [[13-tf.shape(English)[0] ,0], tf.constant([0,0])]\n",
    "    English = tf.pad(English, paddings, \"CONSTANT\")\n",
    "    English = tf.reshape(English, [13, 128])\n",
    "    return English, German \n",
    "\n",
    "# Map this function over the datasets\n",
    "Training_dataset = Training_dataset.map(Pad_English_sentences)\n",
    "Test_dataset = Test_dataset.map(Pad_English_sentences)\n",
    "\n",
    "Training_dataset = Training_dataset.batch(16, drop_remainder=True)\n",
    "Test_dataset = Test_dataset.batch(16, drop_remainder=True)\n",
    "\n",
    "Training_dataset.element_spec\n",
    "Test_dataset.element_spec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: End token embedding.\n",
    "\n",
    "In this section, we will create a custom layer to add the learned end token embedding to the encoder model.\n",
    "\n",
    "![Encoder schematic](Images/neural_translation_model_encoder.png)\n",
    "\n",
    "More specifically we will create a custom layer that takes a batch of English data examples from one of the Datasets, and adds a learned embedded ‘end’ token to the end of each sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer to add the 'end' token\n",
    "\n",
    "class EndTokenLayer(Layer):\n",
    "\n",
    "    def __init__(self, embedding_dim=128, **kwargs):\n",
    "        super(EndTokenLayer, self).__init__(**kwargs)\n",
    "        self.end_token_embedding = tf.Variable(initial_value=tf.random.uniform(shape=(embedding_dim,)), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        end_token = tf.tile(tf.reshape(self.end_token_embedding, shape=(1, 1, self.end_token_embedding.shape[0])),\n",
    "                            [tf.shape(inputs)[0],1,1])\n",
    "        \n",
    "        return tf.keras.layers.concatenate([inputs, end_token], axis=1)  \n",
    "\n",
    "End_Token_Layer = EndTokenLayer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Build the encoder network.\n",
    "The encoder network follows the schematic diagram above. We will now build the RNN encoder model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder\n",
    "\n",
    "inputs = Input(shape=(13,128))\n",
    "x = End_Token_Layer(inputs)\n",
    "x = tf.keras.layers.Masking(mask_value=0.0)(x)\n",
    "LSTM_output, hidden_state, cell_states = tf.keras.layers.LSTM(units = 512, return_sequences=True, return_state=True)(x)\n",
    "outputs = [hidden_state, cell_states]\n",
    "Encoder = tf.keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Build the decoder network\n",
    "The decoder network follows the schematic diagram below. \n",
    "\n",
    "![Decoder schematic](Images/neural_translation_model_decoder.png)\n",
    "\n",
    "More specifically, it will be composed of:\n",
    "\n",
    "* An Embedding layer with vocabulary size set to the number of unique German tokens, embedding dimension 128, and set to mask zero values in the input.\n",
    "* An LSTM layer with 512 units, that returns its hidden and cell states, and also returns sequences.\n",
    "* A Dense layer with number of units equal to the number of unique German tokens, and no activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the decoder\n",
    "\n",
    "class Decoder1(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder1, self).__init__(**kwargs)\n",
    "\n",
    "        self.embLayer = Embedding(input_dim = num_german_tokens+1, output_dim=128, mask_zero=True)\n",
    "        self.lstmLayer = LSTM(units=512, return_state=True, return_sequences=True)\n",
    "        self.denseLayer = Dense(units = num_german_tokens+1, activation=None)\n",
    "\n",
    "    def call(self, inputs, hidden_state=None, cell_state=None):\n",
    "        \n",
    "        x = self.embLayer(inputs)\n",
    "\n",
    "        if (hidden_state is None) or (cell_state is None):\n",
    "            \n",
    "            x, hidden_state, cell_state = self.lstmLayer(x)\n",
    "        else:\n",
    "            x, hidden_state, cell_state = self.lstmLayer(x, initial_state=(hidden_state, cell_state))\n",
    "        \n",
    "        x = self.denseLayer(x)\n",
    "\n",
    "        return x, hidden_state, cell_state\n",
    "Decoder = Decoder1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8: Training loop\n",
    "Let's now write a custom training loop as our model is a bit complex. Here, we will:\n",
    "\n",
    "* Define a function that takes a Tensor batch of German data (as extracted from the training Dataset), and returns a tuple containing German inputs and outputs for the decoder model (refer to schematic diagram above).\n",
    "* Define a function that computes the forward and backward pass for your translation model. More specifically, it will:\n",
    "    * Pass the English input into the encoder, to get the hidden and cell states of the encoder LSTM.\n",
    "    * These hidden and cell states are then passed into the decoder, along with the German inputs, which returns a sequence of outputs (the hidden and cell state outputs of the decoder LSTM are unused in this function).\n",
    "    * The loss should then be computed between the decoder outputs and the German output function argument.\n",
    "    * The function returns the loss and gradients with respect to the encoder and decoder’s trainable variables.\n",
    "* Define and run a custom training loop for a number of epochs (for you to choose) that does the following:\n",
    "    * Iterates through the training dataset, and creates decoder inputs and outputs from the German sequences.\n",
    "    * Updates the parameters of the translation model using the gradients of the function above and an optimizer object.\n",
    "    * Every epoch, compute the validation loss on a number of batches from the validation and save the epoch training and validation losses.\n",
    "* Plot the learning curves for loss vs epoch for both training and validation sets (I recommend to do that all the time, make sure everything is looking alright ^^)\n",
    "\n",
    "This model is computationally demanding to train. If you have a toaster like me, I really recommend using the GPU accelerator hardware on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the inputs and outputs to train the decoder\n",
    "\n",
    "def German_input_output(GermanData):\n",
    "    inputs = GermanData[:,:-1]\n",
    "    outputs = GermanData[:,1:]\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "# Define the optimizer and loss\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compute forward and backward propagation\n",
    "\n",
    "@tf.function\n",
    "def grad(English_inputs, German_inputs, German_outputs):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        hs, cs = Encoder(English_inputs)\n",
    "        Decoder_output, _, _ = Decoder(German_inputs, hs, cs)\n",
    "        loss_value = loss(German_outputs, Decoder_output) \n",
    "      \n",
    "    return (loss_value, tape.gradient(loss_value, Encoder.trainable_variables + Decoder.trainable_variables))\n",
    "\n",
    "def train_translator(num_epochs = 5):\n",
    " \n",
    "    train_loss_results = []\n",
    "    validation_loss_results = []\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        Val_epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        #Training loop\n",
    "        for x, y in Training_dataset:\n",
    "            #Optimize the model\n",
    "            \n",
    "            German_inputs, German_outputs = German_input_output(y)\n",
    "            \n",
    "            loss_value, grads = grad(English_inputs = x, German_inputs=German_inputs, German_outputs = German_outputs)\n",
    "            optimizer.apply_gradients(zip(grads, Encoder.trainable_variables + Decoder.trainable_variables))\n",
    "        \n",
    "            #Compare current loss\n",
    "            epoch_loss_avg(loss_value)\n",
    "            \n",
    "        \n",
    "        #Validation loop\n",
    "        for x, y in Test_dataset:\n",
    "            \n",
    "            German_inputs, German_outputs = German_input_output(y)\n",
    "            validation_loss_value, _ = grad(English_inputs = x, German_inputs=German_inputs, German_outputs = German_outputs)\n",
    "            \n",
    "            Val_epoch_loss_avg(validation_loss_value)\n",
    "            \n",
    "            \n",
    "        train_loss_results.append(epoch_loss_avg.result().numpy())\n",
    "        validation_loss_results.append(Val_epoch_loss_avg.result().numpy())\n",
    "        \n",
    "        \n",
    "        print(\"Epoch {:03d}: Training loss: {:.3f}, Validation loss: {:.3f}\".format(epoch, epoch_loss_avg.result(), Val_epoch_loss_avg.result()))\n",
    "    \n",
    "    return train_loss_results, validation_loss_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNND HERE WE GO: LET'S TRAIN\n",
    "\n",
    "Train_Loss, Validation_Loss = train_translator(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: if you want to look at how the training went\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(12, 5))\n",
    "\n",
    "axes[0].set_xlabel(\"Epochs\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Train_Loss\", fontsize=14)\n",
    "axes[0].set_title('Loss vs epochs')\n",
    "axes[0].plot(Train_Loss)\n",
    "\n",
    "axes[1].set_title('Loss vs epochs')\n",
    "axes[1].set_ylabel(\"Validation_Loss\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epochs\", fontsize=14)\n",
    "axes[1].plot(Validation_Loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we are! Enter your sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER THE SENTENCE YOU WANT TO TRANSLATE! Make sure it doesn't contain more than 13 words as it is the padding size we chose\n",
    "\n",
    "Sentence_to_translate = \"Hello I am Tom!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence_preprocessed = preprocess_sentence(Sentence_to_translate)\n",
    "Sentence_splited = tf.strings.split(Sentence_preprocessed)\n",
    "Sentence_embedded = embedding_layer(Sentence_splited) \n",
    "paddings = [[13-tf.shape(Sentence_embedded)[0] ,0], tf.constant([0,0])]\n",
    "Sentence_padded = tf.pad(Sentence_embedded, paddings, \"CONSTANT\")\n",
    "Sentence_padded = tf.reshape(Sentence_padded, [13, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the encoder\n",
    "Sentence_padded = np.expand_dims(Sentence_padded, 0)\n",
    "hs_cs = Encoder(Sentence_padded)\n",
    "\n",
    "# And finally the decoder\n",
    "\n",
    "index_word = json.loads(tokenizer_config['index_word'])\n",
    "a = np.expand_dims(np.array([1]),0)\n",
    "\n",
    "Prediction, h0, c0 = Decoder(a, hs_cs[0], hs_cs[1])\n",
    "index_firstword_predicted = np.argmax(Prediction[0,0,:], axis=0)\n",
    "index_firstword_predicted = np.expand_dims(np.array([index_firstword_predicted]),0)\n",
    "predicted_sentence = [index_firstword_predicted]\n",
    "\n",
    "for x in range(9):\n",
    "    Prediction, h0, c0 = Decoder(predicted_sentence[-1], h0, c0)\n",
    "    index_word_predicted = np.argmax(Prediction[0,0,:], axis=0)\n",
    "        \n",
    "    if index_word_predicted == 2:\n",
    "        index_word_predicted = np.expand_dims(np.array([index_word_predicted]),0)\n",
    "        predicted_sentence.append(index_word_predicted)\n",
    "        break\n",
    "         \n",
    "    index_word_predicted = np.expand_dims(np.array([index_word_predicted]),0)\n",
    "    predicted_sentence.append(index_word_predicted)\n",
    "\n",
    "    \n",
    "Translation =[]\n",
    "for x in predicted_sentence:\n",
    "    word_predicted = index_word[str(x[0][0])]\n",
    "    Translation.append(word_predicted)\n",
    "\n",
    "print('Here is your translation:')\n",
    "print(' '.join(Translation[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
